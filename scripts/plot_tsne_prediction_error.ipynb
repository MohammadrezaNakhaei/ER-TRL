{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import pickle \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse \n",
    "\n",
    "from tabulate import tabulate \n",
    "\n",
    "from common import *\n",
    "from hydra import initialize, compose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from launch_experiment import initialize as init_agent\n",
    "import rlkit.torch.pytorch_util as ptu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(env, algo='focal'):\n",
    "    with initialize(version_base=\"1.3\", config_path=\"../cfgs\", ):\n",
    "        cfg = compose('experiment', overrides=[f'+env={env}', '+algo=focal'])\n",
    "    agent = init_agent(cfg)\n",
    "    return agent, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(agent, env_name, train_idx=(0, 4, 9, 15, 18), moderate_idx=(0,5,9), batch_size=256):\n",
    "    indices = [task_id_conv[env_name]['train'][idx] for idx in train_idx]\n",
    "    moderate_indices = [task_id_conv[env_name]['moderate'][idx] for idx in moderate_idx]\n",
    "    \n",
    "    batches = [ptu.np_to_pytorch_batch(agent.replay_buffer.random_batch(idx, batch_size=batch_size)) for idx in indices]\n",
    "    moderate_batches = [ptu.np_to_pytorch_batch(agent.eval_buffer.random_batch(idx, batch_size=batch_size)) for idx in moderate_indices]\n",
    "    indices.extend(moderate_indices)\n",
    "    batches.extend(moderate_batches)\n",
    "    indices=np.array(indices)\n",
    "    sorted_indices = np.argsort(indices)\n",
    "\n",
    "    context = [agent.unpack_batch(batch, sparse_reward=False) for batch in batches]\n",
    "    # group like elements together\n",
    "    context = [[x[i] for x in context] for i in range(len(context[0]))]\n",
    "    context = [torch.cat(x, dim=0) for x in context] # 5 * self.meta_batch * self.embedding_batch_size * dim(o, a, r, no, t)\n",
    "    # full context consists of [obs, act, rewards, next_obs, terms]\n",
    "    # if dynamics don't change across tasks, don't include next_obs\n",
    "    # don't include terminals in context\n",
    "    if agent.use_next_obs_in_context:\n",
    "        context = torch.cat(context[:-1], dim=2)\n",
    "    else:\n",
    "        context = torch.cat(context[:-2], dim=2)\n",
    "        \n",
    "    indices = indices[sorted_indices]\n",
    "    context = context[sorted_indices]\n",
    "    tasks = []\n",
    "    return context, indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(agent, env_name, algo, seed=0):\n",
    "    encoder = agent.agent.context_encoder \n",
    "    path = f'output/{env_name}/{algo}/seed{seed}/agent.pth'\n",
    "    checkpoint = torch.load(path)\n",
    "    encoder.load_state_dict(checkpoint['context_encoder'])    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(context, encoder):\n",
    "    z = encoder(context)\n",
    "    task, batch, _ = z.shape \n",
    "    z = z.reshape(task*batch, -1).detach().cpu().numpy()\n",
    "    embed = TSNE(n_components=2, ).fit_transform(z)\n",
    "    return embed.reshape(task, batch, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, cfg = load_agent(env='ant-dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, indices = get_context(agent, cfg.env_name, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_x = []\n",
    "for algo in algo_names:\n",
    "    encoder = load_encoder(agent, cfg.env_name, algo,)\n",
    "    x = embed(context, encoder)\n",
    "    total_x.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"] = 400\n",
    "plt.rcParams[\"font.size\"] = 19\n",
    "plt.rcParams[\"legend.fontsize\"] = 19\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "cm = plt.get_cmap('Set1')\n",
    "colors = cm(np.linspace(0, 1, 8))\n",
    "for ind, x in enumerate(total_x):\n",
    "    ind_x, ind_y = ind%2, ind//2\n",
    "\n",
    "    title = algo_names[ind]\n",
    "    axs[ind_x, ind_y].title.set_text(title)\n",
    "    lines = []\n",
    "    labels = []\n",
    "    for i in range(x.shape[0]):\n",
    "        agent.env.reset_task(indices[i])\n",
    "        goal = np.round(agent.env._goal, 2)\n",
    "        mode = agent.env.get_mode()\n",
    "        label = f'{goal} ({mode})'\n",
    "        l = axs[ind_x, ind_y].scatter(x[i,:,0], x[i,:,1], s=20, label=label, alpha=0.6, color=colors[i])\n",
    "        lines.append(l)\n",
    "        labels.append(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "axs[-1,-1].axes.get_xaxis().set_visible(False)\n",
    "axs[-1,-1].axes.get_yaxis().set_visible(False)\n",
    "axs[-1,-1].spines['top'].set_visible(False)\n",
    "axs[-1,-1].spines['right'].set_visible(False)\n",
    "axs[-1,-1].spines['bottom'].set_visible(False)\n",
    "axs[-1,-1].spines['left'].set_visible(False)\n",
    "axs[-1, -1].legend(lines, labels, loc='center left', ncol=1, )\n",
    "# plt.legend(loc='upper left', ncol=2, bbox_to_anchor=(0, 1.5))\n",
    "#axs[0].set_ylabel(env_names[env_name], fontsize=25)\n",
    "# plt.savefig('figs/two_col_Ant-DIR.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(agent, env_name, context, algo):\n",
    "    encoder = load_encoder(agent, env_name, algo)\n",
    "    z = encoder(context)\n",
    "    z = z.detach().cpu().numpy()\n",
    "    goals = []\n",
    "    for ind in indices:\n",
    "        agent.env.reset_task(ind)\n",
    "        goal = agent.env._goal \n",
    "        goals.append(goal)\n",
    "    goals = np.array(goals)\n",
    "    if goals.ndim==1:\n",
    "        goals = goals.reshape(-1,1)\n",
    "    task, n_sample, _ = z.shape\n",
    "    goals = np.repeat(goals[:,None,], n_sample, 1)\n",
    "    idx_train = n_sample//5\n",
    "    n_train = n_sample-idx_train\n",
    "    n_test = idx_train \n",
    "    z_train, z_test = z[:, idx_train:, ], z[:, :idx_train,]\n",
    "    goals_train, goals_test = goals[:, idx_train:, ], goals[:, :idx_train,]\n",
    "    z_train = z_train.reshape(task*n_train, -1)\n",
    "    z_test = z_test.reshape(task*n_test, -1)\n",
    "\n",
    "    goals_train = goals_train.reshape(task*n_train, -1)\n",
    "    goals_test = goals_test.reshape(task*n_test, -1)\n",
    "    return z_train, z_test, goals_train, goals_test \n",
    "\n",
    "def svr_model(data):\n",
    "    z_train, z_test, goals_train, goals_test = data\n",
    "    model = SVR(kernel='rbf')\n",
    "    model = MultiOutputRegressor(model)\n",
    "    model.fit(z_train, goals_train)\n",
    "    train_pred, test_pred = model.predict(z_train), model.predict(z_test)\n",
    "    rmse_train = mse(goals_train, train_pred)**0.5 \n",
    "    rmse_test = mse(goals_test, test_pred)**0.5 \n",
    "    return rmse_train, rmse_test\n",
    "    \n",
    "def linear_model(data):\n",
    "    z_train, z_test, goals_train, goals_test = data\n",
    "    model = LinearRegression()\n",
    "    model.fit(z_train, goals_train)\n",
    "    train_pred, test_pred = model.predict(z_train), model.predict(z_test)\n",
    "    rmse_train = mse(goals_train, train_pred)**0.5 \n",
    "    rmse_test = mse(goals_test, test_pred)**0.5 \n",
    "    return rmse_train, rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "linear_rmse_trains = defaultdict(list)\n",
    "linear_rmse_tests = defaultdict(list)\n",
    "\n",
    "svr_rmse_trains = defaultdict(list)\n",
    "svr_rmse_tests = defaultdict(list)\n",
    "\n",
    "for env in ['cheetah-vel', 'ant-goal', 'ant-dir', 'humanoid-dir', \n",
    "            'hopper-mass', 'hopper-friction', 'walker-mass', 'walker-friction']:\n",
    "    \n",
    "    seed = 0\n",
    "    n_samples = 1000\n",
    "    agent, cfg = load_agent(env)\n",
    "    \n",
    "    for algo in algo_names:\n",
    "        res_linear_trains = []\n",
    "        res_linear_tests = []\n",
    "        res_svr_trains = []\n",
    "        res_svr_tests = []\n",
    "        for i in range(5):\n",
    "            context, indices = get_context(agent, cfg.env_name, batch_size=n_samples, train_idx=list(range(20)), moderate_idx=list(range(10)))\n",
    "            data = prepare_data(agent, cfg.env_name, context, algo)\n",
    "            train, test = linear_model(data)\n",
    "            res_linear_trains.append(train)\n",
    "            res_linear_tests.append(test)\n",
    "            train, test = svr_model(data)\n",
    "            res_svr_trains.append(train)\n",
    "            res_svr_tests.append(test)\n",
    "        linear_rmse_trains[algo].append((np.mean(res_linear_trains), np.std(res_linear_trains)))\n",
    "        linear_rmse_tests[algo].append((np.mean(res_linear_tests), np.std(res_linear_tests)))\n",
    "        svr_rmse_trains[algo].append((np.mean(res_svr_trains), np.std(res_svr_trains)))\n",
    "        svr_rmse_tests[algo].append((np.mean(res_svr_tests), np.std(res_svr_tests)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_pred(rsme_dict):\n",
    "    results = {}\n",
    "    results['Environment'] = [env_names[env] for env in env_ids]\n",
    "    results['Model'] = ['\\multirow{8}{*}{Linear Regression}', ]\n",
    "    for key, values in rsme_dict.items():\n",
    "        strings = []\n",
    "        for mean, std in values:\n",
    "            strings.append(f'$ {mean:.4f} \\pm {std:.4f} $')\n",
    "        results[key] = strings\n",
    "    print(tabulate(results, tablefmt='latex_raw', headers=results.keys()))\n",
    "    #return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table_pred(linear_rmse_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table_pred(svr_rmse_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = {\n",
    "    ('linear', 'train'): linear_rmse_trains, \n",
    "    ('linear', 'test'): linear_rmse_tests, \n",
    "    ('svr', 'train'): svr_rmse_trains, \n",
    "    ('svr', 'test'): svr_rmse_trains, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scripts/prediction_results.pkl', 'wb') as f:\n",
    "    pickle.dump(total_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scripts/prediction_results.pkl', 'rb') as f:\n",
    "    total_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table_pred(total_results[('svr'),('test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = total_results[('svr'),('test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "def t_test(mean1, mean2, std1, std2, n=5):\n",
    "    t_value = (mean2-mean1)/(np.sqrt(std1**2/n+std2**2/n))\n",
    "    p_value = t.sf(np.abs(t_value), df=4)\n",
    "    return np.round(p_value,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_t_test(mean_std):\n",
    "    means = dict()\n",
    "    stds = dict()\n",
    "    for key, values in mean_std.items():\n",
    "        means[key] = np.array([val[0] for val in values])\n",
    "        stds[key] = np.array([val[1] for val in values])    \n",
    "    resutls = []\n",
    "    for algo in means.keys():\n",
    "        test_vals = t_test(means[algo], means['ER-TRL'], stds[algo], stds['ER-TRL'])\n",
    "        resutls.append(test_vals)\n",
    "        print(f'comparing {algo} to ER-TRL')\n",
    "        print(t_test(means[algo], means['ER-TRL'], stds[algo], stds['ER-TRL']))\n",
    "        print()\n",
    "    print(np.array(resutls)[:-1,:])\n",
    "    print(np.array(resutls)[:-1,:].max(0)<=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t_test(total_results[('linear'),('test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t_test(total_results[('svr'),('test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test(0.16565, 0.16571,  0.00130,  0.00126)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
