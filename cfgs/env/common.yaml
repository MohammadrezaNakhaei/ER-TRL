# @package _global_
algo_params:
  allow_eval: true
  alpha_init: 500.0
  alpha_lr: 1
  alpha_max: 2000.0
  batch_size: 256
  c_iter: 3
  c_lr: 0.0001
  context_lr: 0.0003
  data_dir: ./data
  discount: 0.99
  divergence_name: kl
  dropout: 0.1
  embedding_batch_size: 64
  embedding_mini_batch_size: 64
  eval_epoch: [100000, 1000000, 50000]
  kl_lambda: 0.1
  max_entropy: true
  max_path_length: 200
  mb_replace: false
  meta_batch: 16
  num_evals: 2
  num_exp_traj_eval: 2
  num_iterations: 100
  num_steps_per_eval: 600
  num_tasks_sample: 5
  num_train_steps_per_itr: 2000
  policy_lr: 0.0003
  policy_mean_reg_weight: 0.001
  policy_pre_activation_weight: 0.0
  policy_std_reg_weight: 0.001
  prediction_loss_weight: 1.0
  qf_lr: 0.0003
  recurrent: false
  replay_buffer_size: 200000
  reward_scale: 5.0
  sample: 1
  save_algorithm: false
  save_environment: false
  save_replay_buffer: false
  soft_target_tau: 0.005
  sparse_rewards: false
  target_divergence: 0.05
  train_alpha: true
  train_epoch: [100000, 1000000, 50000]
  update_post_train: 1
  use_brac: true
  use_information_bottleneck: false
  use_next_obs_in_context: false
  use_value_penalty: false
  vf_lr: 0.0003
  use_club_sa: false
  n_trj: 50
  z_loss_weight: 10
  dump_eval_paths: 0